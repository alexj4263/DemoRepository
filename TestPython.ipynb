{
    "nbformat": 4, 
    "cells": [
        {
            "execution_count": null, 
            "source": "", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "# Use Spark for Python to load data and run SQL queries\nThis notebook introduces basic Spark concepts and helps you to start using Spark for Python.\n\nSome familiarity with Python is recommended.\n\nIn this notebook, you'll use the publicly available **mtcars** data set from *Motor Trend* magazine to learn some basic Python. You'll learn how to load data, create a Spark DataFrame, aggregate data, run mathematical formulas, and run SQL queries against the data.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Table of contents\nThis notebook contains these main sections:\n\n1. [Load a DataFrame](#Load_a_DataFrame)\n2. [Create an SQLContext](#Create_an_SQLContext)\n3. [Create a Spark DataFrame](#Create_a_Spark_DataFrame)\n4. [Aggregate data after grouping by columns](#Aggregate_data_after_grouping_by_columns)\n5. [Operate on columns](#Operate_on_columns)\n6. [Run SQL queries from the Spark DataFrame](#Run_SQL_queries_from_the_Spark_DataFrame)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id='Load_a_DataFrame'></a>\n## 1. Load a DataFrame\nA DataFrame is a distributed collection of data that is organized into named columns. The Python pandas DataFrame that you will create will load an external DataFrame called **mtcars**, which includes observations on the following 11 variables:\n\n`[, 1]\tmpg     Miles / (US) gallon`  \n`[, 2]\tcyl     Number of cylinders`  \n`[, 3]\tdisp\tDisplacement (cu. in.)`  \n`[, 4]\thp      Gross horsepower`  \n`[, 5]\tdrat    Rear axle ratio`  \n`[, 6]\twt      Weight (1000 lbs)`  \n`[, 7]\tqsec    1/4 mile time (seconds)`  \n`[, 8]\tvs      0 = V-engine, 1 = straight engine`  \n`[, 9]\tam      Transmission (0 = automatic, 1 = manual)`  \n`[,10]\tgear    Number of forward gears`  \n`[,11]\tcarb    Number of carburetors`", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "source": "import pandas as pd\nmtcars = pd.read_csv('https://ibm.box.com/shared/static/f1dhhjnzjwxmy2c1ys2whvrgz05d1pui.csv')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "Preview the first 3 rows of the DataFrame by using the `head()` method:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "source": "mtcars.head(3)", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "             car   mpg  cyl  disp   hp  drat     wt   qsec  vs  am  gear  carb\n0      Mazda RX4  21.0    6   160  110  3.90  2.620  16.46   0   1     4     4\n1  Mazda RX4 Wag  21.0    6   160  110  3.90  2.875  17.02   0   1     4     4\n2     Datsun 710  22.8    4   108   93  3.85  2.320  18.61   1   1     4     1", 
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>car</th>\n      <th>mpg</th>\n      <th>cyl</th>\n      <th>disp</th>\n      <th>hp</th>\n      <th>drat</th>\n      <th>wt</th>\n      <th>qsec</th>\n      <th>vs</th>\n      <th>am</th>\n      <th>gear</th>\n      <th>carb</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Mazda RX4</td>\n      <td>21.0</td>\n      <td>6</td>\n      <td>160</td>\n      <td>110</td>\n      <td>3.90</td>\n      <td>2.620</td>\n      <td>16.46</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Mazda RX4 Wag</td>\n      <td>21.0</td>\n      <td>6</td>\n      <td>160</td>\n      <td>110</td>\n      <td>3.90</td>\n      <td>2.875</td>\n      <td>17.02</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Datsun 710</td>\n      <td>22.8</td>\n      <td>4</td>\n      <td>108</td>\n      <td>93</td>\n      <td>3.85</td>\n      <td>2.320</td>\n      <td>18.61</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    }, 
                    "execution_count": 2, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "<a id='Create_an_SQLContext'></a>\n## 2. Create an SQLContext\nTo work with a DataFrame, you need an SQLContext class object, and to create a basic one, all you need is a SparkContext. A SparkContext represents the connection to a Spark cluster, and a SparkContext class object named sc, which has been created for you, is used to initialize the SQLContext:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "source": "sqlContext = SQLContext(sc)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "<a id='Create_a_Spark_DataFrame'></a>\n## 3. Create a Spark DataFrame\nUsing the SQLContext class object and the loaded local DataFrame, create a Spark DataFrame and print the schema, or structure, of the DataFrame:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "source": "sdf = sqlContext.createDataFrame(mtcars) \nsdf.printSchema()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "root\n |-- car: string (nullable = true)\n |-- mpg: double (nullable = true)\n |-- cyl: long (nullable = true)\n |-- disp: double (nullable = true)\n |-- hp: long (nullable = true)\n |-- drat: double (nullable = true)\n |-- wt: double (nullable = true)\n |-- qsec: double (nullable = true)\n |-- vs: long (nullable = true)\n |-- am: long (nullable = true)\n |-- gear: long (nullable = true)\n |-- carb: long (nullable = true)\n\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "Display the content of the Spark DataFrame:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "source": "!pip list --isolated", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "\u001b[31mDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\u001b[0m\nappdirs (1.4.2)\nBabel (2.1.1)\nbackports-abc (0.4)\nbackports.shutil-get-terminal-size (1.0.0)\nbackports.ssl-match-hostname (3.4.0.2)\nbasemap (1.0.7)\nbeautifulsoup4 (4.4.1)\nbiopython (1.66)\nbitarray (0.8.1)\nbokeh (0.10.0)\nbrunel (2.3)\ncdsax-jupyter-extensions (0.1)\ncertifi (2015.11.20.1)\ncognitive-assistant (1.0.30)\nconfigparser (3.5.0)\ncycler (0.9.0)\nCython (0.23.4)\ndebtcollector (1.1.0)\ndecorator (4.0.6)\ndescartes (1.0.1)\ndill (0.2.5)\ndlaas-client (0.1.29)\nentrypoints (0.2.2)\nenum (0.4.6)\nenum34 (1.1.5)\nextension-utils (0.1.590)\nextras (0.0.3)\nfixtures (2.0.0)\nFlask (0.10.1)\nfuncsigs (0.4)\nfunctools32 (3.2.3.post2)\nfuture (0.15.2)\nfutures (3.0.5)\ngeopy (1.11.0)\nibmdbpy (0.1.4)\nip-associations-python-novaclient-ext (0.1)\nipykernel (4.3.1)\nipython (4.2.0)\nipython-genutils (0.1.0)\nipywidgets (5.1.5)\niso8601 (0.1.11)\nitsdangerous (0.24)\nJayDeBeApi (0.2.0)\nJinja2 (2.8)\nJPype1 (0.6.1)\njsonschema (2.5.1)\njupyter-client (4.2.2)\njupyter-console (4.1.1)\njupyter-core (4.1.0)\njupyter-kernel-gateway (1.0.0)\njupyter-pip (0.3)\nKeras (1.2.1)\nkeyring (5.7)\nkeystoneauth1 (2.1.0)\nLasagne (0.2.dev1)\nlazy (1.2)\nlinecache2 (1.0.0)\nlxml (3.5.0)\nMarkupSafe (0.23)\nmatplotlib (1.5.0)\nmaven-artifact (0.1.6)\nmistune (0.7.1)\nmock (1.3.0)\nmonotonic (0.4)\nmpld3 (0.2)\nmplleaflet (0.0.5)\nmsgpack-python (0.4.6)\nMySQL-python (1.2.5)\nnbconvert (4.1.0)\nnbformat (4.0.1)\nnetaddr (0.7.18)\nnetifaces (0.10.4)\nnetworkx (1.10)\nnltk (3.1)\nnose (1.3.7)\nnotebook (4.2.0)\nnumexpr (2.4.6)\nnumpy (1.10.4)\nos-diskconfig-python-novaclient-ext (0.1.2)\nos-networksv2-python-novaclient-ext (0.25)\nos-virtual-interfacesv2-python-novaclient-ext (0.19)\noslo.config (3.1.0)\noslo.i18n (3.1.0)\noslo.serialization (2.1.0)\noslo.utils (3.2.0)\npackaging (16.8)\npandas (0.17.1)\npath.py (8.1.2)\npatsy (0.4.1)\npbr (1.8.1)\npexpect (4.0.1)\npickleshare (0.5)\nPillow (3.0.0)\npip (9.0.1)\npipeline (0.1.370)\npixiedust (1.0.2)\nprettytable (0.7.2)\nprojectnb (0.1.6)\nprotobuf (3.0.0b2)\npsycopg2 (2.5.4)\nptyprocess (0.5)\nPygments (2.0.2)\npyparsing (2.0.6)\npyproj (1.9.4)\npypyodbc (1.3.4)\npyrax (1.9.5)\npyshp (1.2.3)\npython-dateutil (2.4.2)\npython-keystoneclient (2.0.0)\npython-mimeparse (1.5.1)\npython-novaclient (2.35.0)\npython-subunit (1.2.0)\npytz (2015.7)\nPyYAML (3.11)\npyzmq (15.1.0)\nrackspace-auth-openstack (1.3)\nrackspace-novaclient (1.5)\nrax-default-network-flags-python-novaclient-ext (0.3.2)\nrax-scheduled-images-python-novaclient-ext (0.3.1)\nrepoze.lru (0.6)\nrequests (2.9.1)\nscheduler-client (1.0.0)\nscikit-image (0.11.3)\nscikit-learn (0.17)\nscipy (0.17.0)\nsetuptools (34.3.0)\nsilpa-common (0.3)\nsimplegeneric (0.8.1)\nsimplejson (3.8.1)\nsingledispatch (3.4.0.3)\nsix (1.10.0)\nsklearn-pipeline (0.1.93)\nsoundex (1.1.3)\nstatsmodels (0.6.1)\nstevedore (1.10.0)\nstreamsx (1.6.0)\nsympy (0.7.6.1)\nsystemml (0.12.0)\ntensorflow (0.9.0)\nterminado (0.5)\ntestrepository (0.0.20)\ntesttools (2.1.0)\nTheano (0.8.2)\ntornado (4.3)\ntraceback2 (1.4.0)\ntraitlets (4.2.1)\ntransformation (0.1.502)\nunittest2 (1.1.0)\nurllib3 (1.14)\nWerkzeug (0.11.2)\nwheel (0.29.0)\nwidgetsnbextension (1.2.3)\nwrapt (1.10.6)\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "execution_count": null, 
            "source": "sdf.show(32)", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": []
        }, 
        {
            "source": "Try different ways of retrieving subsets of the data. For example, get the first 5 values in the **mpg** column:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "source": "sdf.select('mpg').show(5)", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": []
        }, 
        {
            "source": "Filter the DataFrame to retain only rows with **mpg** values that are less than 18:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "source": "sdf.filter(sdf['mpg'] < 18).show()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "<a id='Aggregate_data_after_grouping_by_columns'></a>\n## 4. Aggregate data after grouping by columns\nSpark DataFrames support a number of common functions to aggregate data after grouping. For example, you can compute the average weight of cars as a function of the number of cylinders:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "source": "sdf.groupby(['cyl'])\\\n.agg({\"wt\": \"AVG\"})\\\n.show()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "You can also sort the output from the aggregation to determine the most popular cylinder configuration in the DataFrame:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "source": "car_counts = sdf.groupby(['cyl'])\\\n.agg({\"wt\": \"count\"})\\\n.sort(\"count(wt)\", ascending=False)\\\n.show()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "<a id='Operate_on_columns'></a>\n## 5. Operate on columns\nPython provides a number of functions that you can apply directly to columns for data processing. In the following example, a basic arithmetic function converts lbs to metric tons:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "source": "sdf.withColumn('wtTon', sdf['wt'] * 0.45).select('car','wt','wtTon').show(6)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "<a id='Run_SQL_queries_from_the_Spark_DataFrame'></a>\n## 6. Run SQL queries from the Spark DataFrame\nYou can register a Spark DataFrame as a temporary table and then run SQL queries over the data. The `sql` function enables an application to run SQL queries programmatically and returns the result as a DataFrame:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "source": "sdf.registerTempTable(\"cars\")\n\nhighgearcars = sqlContext.sql(\"SELECT car, gear FROM cars WHERE gear >= 5\")\nhighgearcars.show()    ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "## That's it!\nYou successfully completed this notebook! You learned how to load a DataFrame, view and filter the data, aggregate the data, perform operations on the data in specific columns, and run SQL queries against the data. For more information about Spark, see the [Spark Quick Start Guide](http://spark.apache.org/docs/latest/quick-start.html).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Want to learn more?\n### Free courses on <a href=\"https://bigdatauniversity.com/courses/?utm_source=tutorial-dashdb-python&utm_medium=github&utm_campaign=bdu/\" rel=\"noopener noreferrer\" target=\"_blank\">Big Data University</a>: <a href=\"https://bigdatauniversity.com/courses/?utm_source=tutorial-dashdb-python&utm_medium=github&utm_campaign=bdu\" rel=\"noopener noreferrer\" target=\"_blank\"><img src = \"https://ibm.box.com/shared/static/xomeu7dacwufkoawbg3owc8wzuezltn6.png\" width=600px> </a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Authors\n\n**Saeed Aghabozorgi**, PhD, is a Data Scientist in IBM with a track record of developing enterprise-level applications that substantially increases clients' ability to turn data into actionable knowledge. He is a researcher in the data mining field and an expert in developing advanced analytic methods like machine learning and statistical modelling on large data sets.\n\n**Polong Lin** is a Data Scientist at IBM in Canada. Under the Emerging Technologies division, Polong is responsible for educating the next generation of data scientists through Big Data University. Polong is a regular speaker in conferences and meetups, and holds an M.Sc. in Cognitive Psychology.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Copyright \u00a9 2016 Big Data University. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\" rel=\"noopener noreferrer\" target=\"_blank\">MIT License</a>.", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "language_info": {
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "mimetype": "text/x-python", 
            "name": "python", 
            "file_extension": ".py", 
            "version": "2.7.11", 
            "nbconvert_exporter": "python"
        }, 
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 2 with Spark 1.6", 
            "name": "python2"
        }
    }, 
    "nbformat_minor": 1
}